{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.AFB.BHE.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.AFB.BHN.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.AFB.BHZ.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.DEB.BHE.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.DEB.BHN.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "working on station Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\\2004.202.00.00.00.000.MX.DEB.BHZ.SAC and trace 1\n",
      "N and Nfft are 188 (proposed 188),18000 (proposed 18000)\n",
      "smoothing source takes 0.0050s\n",
      "receiver: AFB MX BHE\n",
      "read S 0.0050s, cc 0.1079s, write cc 0.0400s\n",
      "receiver: AFB MX BHN\n",
      "read S 0.0050s, cc 0.1029s, write cc 0.0444s\n",
      "receiver: AFB MX BHZ\n",
      "read S 0.0050s, cc 0.1029s, write cc 0.0439s\n",
      "receiver: DEB MX BHE\n",
      "read S 0.0050s, cc 0.1029s, write cc 0.0437s\n",
      "receiver: DEB MX BHN\n",
      "read S 0.0050s, cc 0.1019s, write cc 0.0439s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0050s, cc 0.1039s, write cc 0.0440s\n",
      "smoothing source takes 0.0070s\n",
      "receiver: AFB MX BHN\n",
      "read S 0.0070s, cc 0.1029s, write cc 0.0440s\n",
      "receiver: AFB MX BHZ\n",
      "read S 0.0070s, cc 0.1039s, write cc 0.0436s\n",
      "receiver: DEB MX BHE\n",
      "read S 0.0070s, cc 0.1029s, write cc 0.0441s\n",
      "receiver: DEB MX BHN\n",
      "read S 0.0070s, cc 0.0989s, write cc 0.0446s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0070s, cc 0.1039s, write cc 0.0433s\n",
      "smoothing source takes 0.0050s\n",
      "receiver: AFB MX BHZ\n",
      "read S 0.0050s, cc 0.1079s, write cc 0.0452s\n",
      "receiver: DEB MX BHE\n",
      "read S 0.0050s, cc 0.1049s, write cc 0.0460s\n",
      "receiver: DEB MX BHN\n",
      "read S 0.0050s, cc 0.1079s, write cc 0.0452s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0050s, cc 0.1049s, write cc 0.0478s\n",
      "smoothing source takes 0.0050s\n",
      "receiver: DEB MX BHE\n",
      "read S 0.0050s, cc 0.1429s, write cc 0.0555s\n",
      "receiver: DEB MX BHN\n",
      "read S 0.0050s, cc 0.1369s, write cc 0.0555s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0050s, cc 0.1359s, write cc 0.0536s\n",
      "smoothing source takes 0.0050s\n",
      "receiver: DEB MX BHN\n",
      "read S 0.0050s, cc 0.1359s, write cc 0.0530s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0050s, cc 0.1359s, write cc 0.0530s\n",
      "smoothing source takes 0.0050s\n",
      "receiver: DEB MX BHZ\n",
      "read S 0.0050s, cc 0.1319s, write cc 0.0530s\n",
      "unreadable garbarge 14919\n",
      "it takes   5.23s to process the chunk of Z:\\Thesis_mimi\\CC_Najib\\Data Stasiun\n",
      "it takes   5.23s to process step 1 in total\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import scipy\n",
    "import obspy\n",
    "import pyasdf\n",
    "import datetime\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import noise_module\n",
    "from mpi4py import MPI\n",
    "from scipy.fftpack.helper import next_fast_len\n",
    "import matplotlib.pyplot  as plt\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "'''\n",
    "This main script of NoisePy:\n",
    "    1) read the saved noise data in user-defined chunk of inc_hours, cut them into smaller length segments, do \n",
    "    general pre-processing (trend, normalization) and then do FFT;\n",
    "    2) save all FFT data of the same time chunk in memory;\n",
    "    3) performs cross-correlation for all station pairs in the same time chunk and output the sub-stacked (if \n",
    "    selected) into ASDF format;\n",
    "\n",
    "Authors: Chengxin Jiang (chengxin_jiang@fas.harvard.edu)\n",
    "         Marine Denolle (mdenolle@fas.harvard.edu)\n",
    "        \n",
    "NOTE:\n",
    "    0. MOST occasions you just need to change parameters followed with detailed explanations to run the script. \n",
    "    1. To read SAC/mseed files, we assume the users have sorted the data by the time chunk they prefer (e.g., 1day) \n",
    "        and store them in folders named after the time chunk (e.g, 2010_10_1). modify L135 to find your local data; \n",
    "    2. A script of S0B_to_ASDF.py is provided to help clean messy SAC/MSEED data and convert them into ASDF format.\n",
    "        the script takes minor time compared to that for cross-correlation. so we recommend to use S0B script for\n",
    "        better NoisePy performance. the downside is that it duplicates the continuous noise data on your machine;\n",
    "    3. When \"coherency\" is preferred, please set \"freq_norm\" to \"rma\" and \"time_norm\" to \"no\" for better performance.\n",
    "'''\n",
    "\n",
    "tt0=time.time()\n",
    "\n",
    "########################################\n",
    "#########PARAMETER SECTION##############\n",
    "########################################\n",
    "\n",
    "# absolute path parameters\n",
    "rootpath  = r'Z:\\Thesis_mimi\\CC_Najib'                                      # root path for this data processing\n",
    "CCFDIR    = os.path.join(rootpath,'CCF')                                    # dir to store CC data\n",
    "DATADIR   = os.path.join(rootpath,'Data Stasiun')                               # dir where noise data is located\n",
    "local_data_path = os.path.join(rootpath,'Data Stasiun')                         # absolute dir where SAC files are stored: this para is VERY IMPORTANT and has to be RIGHT if input_fmt is not h5 for asdf!!!\n",
    "locations = os.path.join(DATADIR,'station.txt')                             # station info including network,station,channel,latitude,longitude,elevation: only needed when input_fmt is not h5 for asdf\n",
    "\n",
    "# some control parameters\n",
    "input_fmt   = 'sac'                                                          # string: 'h5', 'sac','mseed' \n",
    "freq_norm   = 'rma'                                                         # 'no' for no whitening, or 'rma' for running-mean average, 'phase_only' for sign-bit normalization in freq domain.\n",
    "time_norm   = 'no'                                                          # 'no' for no normalization, or 'rma', 'one_bit' for normalization in time domain\n",
    "cc_method   = 'xcorr'                                                       # 'xcorr' for pure cross correlation, 'deconv' for deconvolution; FOR \"COHERENCY\" PLEASE set freq_norm to \"rma\", time_norm to \"no\" and cc_method to \"xcorr\"\n",
    "flag        = True                                                          # print intermediate variables and computing time for debugging purpose\n",
    "acorr_only  = False                                                         # only perform auto-correlation \n",
    "xcorr_only  = True                                                          # only perform cross-correlation or not\n",
    "ncomp       = 2                                                             # 1 or 3 component data (needed to decide whether do rotation)\n",
    "\n",
    "# station/instrument info for input_fmt=='sac' or 'mseed'\n",
    "stationxml = False                                                          # station.XML file used to remove instrument response for SAC/miniseed data\n",
    "rm_resp   = 'no'                                                            # select 'no' to not remove response and use 'inv','spectrum','RESP', or 'polozeros' to remove response\n",
    "respdir   = os.path.join(rootpath,'no')                                   # directory where resp files are located (required if rm_resp is neither 'no' nor 'inv')\n",
    "# read station list\n",
    "if input_fmt != 'h5':\n",
    "    if not os.path.isfile(locations): \n",
    "        raise ValueError('Abort! station info is needed for this script')   \n",
    "    locs = pd.read_csv(locations)\n",
    "    \n",
    "\n",
    "# pre-processing parameters \n",
    "cc_len    = 1800                                                            # basic unit of data length for fft (sec)\n",
    "step      = 450                                                             # overlapping between each cc_len (sec)\n",
    "smooth_N  = 10                                                              # moving window length for time/freq domain normalization if selected (points)\n",
    "\n",
    "# cross-correlation parameters\n",
    "maxlag         = 100                                                        # lags of cross-correlation to save (sec)\n",
    "substack       = True                                                       # True = smaller stacks within the time chunk. False: it will stack over inc_hours\n",
    "                                                                            # for instance: substack=True, substack_len=cc_len means that you keep ALL of the correlations\n",
    "                                                                            # if substack=True, substack_len=2*cc_len, then you pre-stack every 2 correlation windows.\n",
    "substack_len   = cc_len                                                     # how long to stack over (for monitoring purpose): need to be multiples of cc_len\n",
    "smoothspect_N  = 10                                                         # moving window length to smooth spectrum amplitude (points)\n",
    "\n",
    "# criteria for data selection\n",
    "max_over_std = 1000                                                           # threahold to remove window of bad signals: set it to 10*9 if prefer not to remove them\n",
    "\n",
    "# maximum memory allowed per core in GB\n",
    "MAX_MEM = 4.0\n",
    "\n",
    "# load useful download info if start from ASDF\n",
    "if input_fmt == 'h5':\n",
    "    dfile = os.path.join(DATADIR,'fft_cc_data.txt')\n",
    "    down_info = eval(open(dfile).read())\n",
    "    samp_freq = down_info['samp_freq']\n",
    "    freqmin   = down_info['freqmin']\n",
    "    freqmax   = down_info['freqmax']\n",
    "    start_date = down_info['start_date']\n",
    "    end_date   = down_info['end_date']\n",
    "    inc_hours  = down_info['inc_hours']  \n",
    "    ncomp      = down_info['ncomp'] \n",
    "else:   # sac or mseed format\n",
    "    samp_freq = 10\n",
    "    freqmin   = 0.01\n",
    "    freqmax   = 10\n",
    "    start_date = [\"2004.202.00.00.00.000\"]\n",
    "    end_date   = [\"2004.202.00.00.00.000\"]\n",
    "    inc_hours  = 24\n",
    "dt = 1/samp_freq\n",
    "\n",
    "##################################################\n",
    "# we expect no parameters need to be changed below\n",
    "\n",
    "# make a dictionary to store all variables: also for later cc\n",
    "fc_para={'samp_freq':samp_freq,\n",
    "         'dt':dt,\n",
    "         'cc_len':cc_len,\n",
    "         'step':step,\n",
    "         'freqmin':freqmin,\n",
    "         'freqmax':freqmax,\n",
    "         'freq_norm':freq_norm,\n",
    "         'time_norm':time_norm,\n",
    "         'cc_method':cc_method,\n",
    "         'smooth_N':smooth_N,\n",
    "         'rootpath':rootpath,\n",
    "         'CCFDIR':CCFDIR,\n",
    "         'start_date':start_date[0],\n",
    "         'end_date':end_date[0],\n",
    "         'inc_hours':inc_hours,\n",
    "         'substack':substack,\n",
    "         'substack_len':substack_len,\n",
    "         'smoothspect_N':smoothspect_N,\n",
    "         'maxlag':maxlag,\n",
    "         'max_over_std':max_over_std,\n",
    "         'ncomp':ncomp,\n",
    "         'stationxml':stationxml,\n",
    "         'rm_resp':rm_resp,\n",
    "         'respdir':respdir,\n",
    "         'input_fmt':input_fmt}\n",
    "# save fft metadata for future reference\n",
    "fc_metadata  = os.path.join(CCFDIR,'fft_cc_data.txt')       \n",
    "\n",
    "#######################################\n",
    "###########PROCESSING SECTION##########\n",
    "#######################################\n",
    "\n",
    "#--------MPI---------\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    if not os.path.isdir(CCFDIR):os.mkdir(CCFDIR)\n",
    "    \n",
    "    # save metadata \n",
    "    fout = open(fc_metadata,'w')\n",
    "    fout.write(str(fc_para));fout.close()\n",
    "\n",
    "    # set variables to broadcast\n",
    "    if input_fmt == 'h5':\n",
    "        tdir = sorted(glob.glob(os.path.join(DATADIR,'*.h5')))\n",
    "        \n",
    "    else:\n",
    "        tdir = sorted(glob.glob(local_data_path))\n",
    "        if len(tdir)==0: raise ValueError('No data file in %s',DATADIR)\n",
    "        # get nsta by loop through all event folder\n",
    "        nsta = 0\n",
    "        for ii in range(len(tdir)):\n",
    "            tnsta = len(glob.glob(os.path.join(tdir[ii],'*'+input_fmt)))\n",
    "            if nsta<tnsta:nsta=tnsta\n",
    "\n",
    "    nchunk = len(tdir)\n",
    "    splits  = nchunk\n",
    "    if nchunk==0:\n",
    "        raise IOError('Abort! no available seismic files for FFT')\n",
    "else:\n",
    "    if input_fmt == 'h5':\n",
    "        splits,tdir = [None for _ in range(2)]\n",
    "    else: splits,tdir,nsta = [None for _ in range(3)]\n",
    "\n",
    "# broadcast the variables\n",
    "splits = comm.bcast(splits,root=0)\n",
    "tdir  = comm.bcast(tdir,root=0)\n",
    "if input_fmt != 'h5': nsta = comm.bcast(nsta,root=0)\n",
    "\n",
    "# MPI loop: loop through each user-defined time chunk\n",
    "for ick in range (rank,splits,size):\n",
    "    t10=time.time()   \n",
    "\n",
    "    #############LOADING NOISE DATA AND DO FFT##################\n",
    "\n",
    "    # get the tempory file recording cc process\n",
    "    if input_fmt == 'h5':\n",
    "        tmpfile = os.path.join(CCFDIR,tdir[ick].split('/')[-1].split('.')[0]+'.tmp')\n",
    "        \n",
    "    else: \n",
    "        tmpfile = os.path.join(CCFDIR,tdir[ick].split('/')[-1]+'.tmp')\n",
    "    \n",
    "    # check whether time chunk been processed or not\n",
    "    if os.path.isfile(tmpfile):\n",
    "        ftemp = open(tmpfile,'r')\n",
    "        alines = ftemp.readlines()\n",
    "        print(alines, \"alines\")\n",
    "        if len(alines) and alines[-1] == 'done':\n",
    "            continue\n",
    "        else:\n",
    "            ftemp.close()\n",
    "            os.remove(tmpfile)\n",
    "    \n",
    "    # retrive station information\n",
    "    if input_fmt == 'h5':\n",
    "        ds=pyasdf.ASDFDataSet(tdir[ick],mpi=False,mode='r') \n",
    "        sta_list = ds.waveforms.list()\n",
    "        nsta=ncomp*len(sta_list)\n",
    "        print('found %d stations in total'%nsta)\n",
    "    else:\n",
    "        sta_list = sorted(glob.glob(os.path.join(tdir[ick],'*'+input_fmt)))\n",
    "    if (len(sta_list)==0):\n",
    "        print('continue! no data in %s'%tdir[ick]);continue\n",
    "    \n",
    "    # crude estimation on memory needs (assume float32)\n",
    "    nsec_chunk = inc_hours/24*86400\n",
    "    nseg_chunk = int(np.floor((nsec_chunk-cc_len)/step))\n",
    "    npts_chunk = int(nseg_chunk*cc_len*samp_freq)\n",
    "    memory_size = nsta*npts_chunk*4/1024**3\n",
    "    if memory_size > MAX_MEM:\n",
    "        raise ValueError('Require %5.3fG memory but only %5.3fG provided)! Reduce inc_hours to avoid this issue!' % (memory_size,MAX_MEM))\n",
    "\n",
    "    nnfft = int(next_fast_len(int(cc_len*samp_freq)))\n",
    "    # open array to store fft data/info in memory\n",
    "    fft_array = np.zeros((nsta,nseg_chunk*(nnfft//2)),dtype=np.complex64)\n",
    "    fft_std   = np.zeros((nsta,nseg_chunk),dtype=np.float32)\n",
    "    fft_flag  = np.zeros(nsta,dtype=np.int16)\n",
    "    fft_time  = np.zeros((nsta,nseg_chunk),dtype=np.float64) \n",
    "    # station information (for every channel)\n",
    "    station=[];network=[];channel=[];clon=[];clat=[];location=[];elevation=[]     \n",
    "\n",
    "    # loop through all stations\n",
    "    iii = 0\n",
    "    for ista in range(len(sta_list)):\n",
    "        tmps = sta_list[ista]\n",
    "\n",
    "        if input_fmt == 'h5':\n",
    "            # get station and inventory\n",
    "            try:\n",
    "                inv1 = ds.waveforms[tmps]['StationXML']\n",
    "            except Exception as e:\n",
    "                print('abort! no stationxml for %s in file %s'%(tmps,tdir[ick]))\n",
    "                continue\n",
    "            sta,net,lon,lat,loc = noise_module.sta_info_from_inv(inv1)\n",
    "\n",
    "            # get days information: works better than just list the tags \n",
    "            all_tags = ds.waveforms[tmps].get_waveform_tags()\n",
    "            if len(all_tags)==0:continue\n",
    "        \n",
    "        else: # get station information\n",
    "            all_tags = [1]\n",
    "            sta = tmps.split('/')[-1]\n",
    "       \n",
    "        #----loop through each stream----\n",
    "        for itag in range(len(all_tags)):\n",
    "            if flag:print(\"working on station %s and trace %s\" % (sta,all_tags[itag]))\n",
    "            \n",
    "            # read waveform data\n",
    "            if input_fmt == 'h5':\n",
    "                source = ds.waveforms[tmps][all_tags[itag]]\n",
    "            else:\n",
    "                source = obspy.read(tmps)\n",
    "                source[0].stats.sac[\"stel\"] = 0 #elevation\n",
    "                source[0].stats.sac[\"cmpaz\"] = 0 #azimuth\n",
    "                source[0].stats.sac[\"cmpinc\"] = 0\n",
    "                inv1   = noise_module.stats2inv(source[0].stats,fc_para,locs)\n",
    "                a = noise_module.sta_info_from_inv(inv1)\n",
    "                # ('AFB', 'MX', 109.77509307861328, -7.626959800720215, 0.0, '00')\n",
    "                sta,net,lon,lat,loc, el = noise_module.sta_info_from_inv(inv1)\n",
    "\n",
    "            # channel info \n",
    "            comp = source[0].stats.channel\n",
    "            if comp[-1] =='U': comp.replace('U','Z')\n",
    "            if len(source)==0:continue\n",
    "\n",
    "            # cut daily-long data into smaller segments (dataS always in 2D)\n",
    "            trace_stdS,dataS_t,dataS = noise_module.cut_trace_make_stat(fc_para,source)        # optimized version:3-4 times faster\n",
    "            if not len(dataS): continue\n",
    "            N = dataS.shape[0]\n",
    "\n",
    "            # do normalization if needed\n",
    "            source_white = noise_module.noise_processing(fc_para,dataS)\n",
    "            Nfft = source_white.shape[1];Nfft2 = Nfft//2\n",
    "            if flag:print('N and Nfft are %d (proposed %d),%d (proposed %d)' %(N,nseg_chunk,Nfft,nnfft))\n",
    "\n",
    "            # keep track of station info to write into parameter section of ASDF files\n",
    "            station.append(sta)\n",
    "            network.append(net)\n",
    "            channel.append(comp)\n",
    "            clon.append(lon)\n",
    "            clat.append(lat)\n",
    "            location.append(loc)\n",
    "            #elevation.append(elv)\n",
    "\n",
    "            # load fft data in memory for cross-correlations\n",
    "            data = source_white[:,:Nfft2]\n",
    "            fft_array[iii] = data.reshape(data.size)\n",
    "            fft_std[iii]   = trace_stdS\n",
    "            fft_flag[iii]  = 1\n",
    "            fft_time[iii]  = dataS_t\n",
    "            iii+=1\n",
    "            del trace_stdS,dataS_t,dataS,source_white,data\n",
    "    \n",
    "    if input_fmt == 'h5': del ds\n",
    "\n",
    "    # check whether array size is enough\n",
    "    if iii!=nsta:\n",
    "        print('it seems some stations miss data in download step, but it is OKAY!')\n",
    "\n",
    "    #############PERFORM CROSS-CORRELATION##################\n",
    "    ftmp = open(tmpfile,'w')\n",
    "    # make cross-correlations \n",
    "    for iiS in range(iii):\n",
    "        fft1 = fft_array[iiS]\n",
    "        source_std = fft_std[iiS]\n",
    "        sou_ind = np.where((source_std<fc_para['max_over_std'])&(source_std>0)&(np.isnan(source_std)==0))[0]\n",
    "        if not fft_flag[iiS] or not len(sou_ind): continue\n",
    "                \n",
    "        t0=time.time()\n",
    "        #-----------get the smoothed source spectrum for decon later----------\n",
    "        sfft1 = noise_module.smooth_source_spect(fc_para,fft1)\n",
    "        sfft1 = sfft1.reshape(N,Nfft2)\n",
    "        t1=time.time()\n",
    "        if flag: \n",
    "            print('smoothing source takes %6.4fs' % (t1-t0))\n",
    "\n",
    "        # get index right for auto/cross correlation\n",
    "        istart=iiS;iend=iii\n",
    "#             if ncomp==1:\n",
    "#                 iend=np.minimum(iiS+ncomp,iii)\n",
    "#             else:\n",
    "#                 if (channel[iiS][-1]=='Z'): # THIS IS NOT GENERALIZABLE. WE need to change this to the order there are bugs that shifts the components\n",
    "#                     iend=np.minimum(iiS+1,iii)\n",
    "#                 elif (channel[iiS][-1]=='N'):\n",
    "#                     iend=np.minimum(iiS+2,iii)\n",
    "#                 else:\n",
    "#                     iend=np.minimum(iiS+ncomp,iii)\n",
    "            \n",
    "#         if xcorr_only:\n",
    "#             if ncomp==1:\n",
    "#                 istart=np.minimum(iiS+ncomp,iii)\n",
    "#             else:\n",
    "#                 if (channel[iiS][-1]=='Z'):\n",
    "#                     istart=np.minimum(iiS+1,iii)\n",
    "#                 elif (channel[iiS][-1]=='N'):\n",
    "#                     istart=np.minimum(iiS+2,iii)\n",
    "#                 else:\n",
    "#                     istart=np.minimum(iiS+ncomp,iii)\n",
    "\n",
    "        #-----------now loop III for each receiver B----------\n",
    "        for iiR in range(istart,iend):\n",
    "            if acorr_only:\n",
    "                if (station[iiR]==station[iiS]):continue\n",
    "            \n",
    "            if flag:print('receiver: %s %s %s' % (station[iiR],network[iiR],channel[iiR]))\n",
    "            if not fft_flag[iiR]: continue\n",
    "                \n",
    "            fft2 = fft_array[iiR];sfft2 = fft2.reshape(N,Nfft2)\n",
    "            receiver_std = fft_std[iiR]\n",
    "\n",
    "            #---------- check the existence of earthquakes ----------\n",
    "            rec_ind = np.where((receiver_std<fc_para['max_over_std'])&(receiver_std>0)&(np.isnan(receiver_std)==0))[0]\n",
    "            bb=np.intersect1d(sou_ind,rec_ind)\n",
    "            if len(bb)==0:continue\n",
    "\n",
    "            t2=time.time()\n",
    "            corr,tcorr,ncorr=noise_module.correlate(sfft1[bb,:],sfft2[bb,:],fc_para,Nfft,fft_time[iiR][bb])\n",
    "            t3=time.time()\n",
    "\n",
    "            #---------------keep daily cross-correlation into a hdf5 file--------------\n",
    "            if input_fmt == 'h5':\n",
    "                tname = tdir[ick].split('/')[-1]\n",
    "            else: \n",
    "                tname = tdir[ick].split('/')[-1]+'.h5'\n",
    "            cc_h5 = os.path.join(CCFDIR,tname)\n",
    "            crap  = np.zeros(corr.shape,dtype=corr.dtype)\n",
    "\n",
    "            with pyasdf.ASDFDataSet(cc_h5,mpi=False) as ccf_ds:\n",
    "                coor = {'lonS':clon[iiS],\n",
    "                        'latS':clat[iiS],\n",
    "                        'lonR':clon[iiR],\n",
    "                        'latR':clat[iiR]}\n",
    "                comp = channel[iiS][-1]+channel[iiR][-1]\n",
    "                parameters = noise_module.cc_parameters(fc_para,coor,tcorr,ncorr,comp)\n",
    "\n",
    "                # source-receiver pair\n",
    "                data_type = network[iiS]+'.'+station[iiS]+'_'+network[iiR]+'.'+station[iiR]\n",
    "                path = channel[iiS]+'_'+channel[iiR]\n",
    "                crap[:] = corr[:]\n",
    "                ccf_ds.add_auxiliary_data(data=crap, \n",
    "                                          data_type=data_type, \n",
    "                                          path=path, \n",
    "                                          parameters=parameters)\n",
    "                ftmp.write(network[iiS]+'.'+station[iiS]+'.'+channel[iiS]+'_'+network[iiR]+'.'+station[iiR]+'.'+channel[iiR]+'\\n')\n",
    "\n",
    "            t4=time.time()\n",
    "            if flag:print('read S %6.4fs, cc %6.4fs, write cc %6.4fs'% ((t1-t0),(t3-t2),(t4-t3)))\n",
    "            \n",
    "            del fft2,sfft2,receiver_std\n",
    "        del fft1,sfft1,source_std\n",
    "\n",
    "    # create a stamp to show time chunk being done\n",
    "    ftmp.write('done')\n",
    "    ftmp.close()\n",
    "\n",
    "    fft_array=[];fft_std=[];fft_flag=[];fft_time=[]\n",
    "    n = gc.collect();print('unreadable garbarge',n)\n",
    "\n",
    "    t11 = time.time()\n",
    "    print('it takes %6.2fs to process the chunk of %s' % (t11-t10,tdir[ick].split('/')[-1]))\n",
    "\n",
    "tt1 = time.time()\n",
    "print('it takes %6.2fs to process step 1 in total' % (tt1-tt0))\n",
    "comm.barrier()\n",
    "\n",
    "# merge all path_array and output\n",
    "if rank == 0:\n",
    "    sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
